\chapter{Model Evaluation}
\label{ch:modelEval}

Compare forecasting performance of different learning method is one of the most important task of prediction study. This chapter introduces the criteria used in this dissertation.\\


let $ \hat{p} $ be the predicted variable, $ p $ be the actual stock price, $ \epsilon = \hat{p} - p $ be the forecast error, and $ N $ be the number of total testing sample number. Popular evaluation measures used in this study includes\cite[Section~2.2, p.~23--24]{poon2005practical},
\begin{itemize}
	\item \textit{Mean Error} (ME)
	\begin{equation}
	ME=\frac{1}{N} \sum_{i=1}^{N}\epsilon_i=\frac{1}{N} \sum_{i=1}^{N} (\hat{p}_i - p_i)
	\end{equation}
	
	\item \textit{Mean Square Error} (MSE)
	\begin{equation}
	MSE = \frac{1}{N} \sum_{i=1}^{N}\epsilon_i^2=\frac{1}{N} \sum_{i=1}^{N} (\hat{p}_i - p_i)^2
	\end{equation}
	
	\item \textit{Root Mean Square Error} (RMSE)
	\begin{equation}
	RMSE = \sqrt{\frac{1}{N} \sum_{i=1}^{N}\epsilon_i^2}=\sqrt{\frac{1}{N} \sum_{i=1}^{N} (\hat{p}_i - p_i)^2}
	\end{equation}
	
	\item \textit{Mean Absolute Error} (MAE)
	\begin{equation}
	MAE=\frac{1}{N} \sum_{i=1}^{N} \lvert \epsilon_i \rvert =\frac{1}{N} \sum_{i=1}^{N} \lvert \hat{p}_i - p_i \rvert
	\end{equation}
	
	\item \textit{Mean Absolute Percent Error} (MAPE)
	\begin{equation}
	MAPE=\frac{1}{N} \sum_{i=1}^{N} \frac{\lvert \epsilon_i \rvert}{p_i} =\frac{1}{N} \sum_{i=1}^{N} \frac{\lvert \hat{p}_i - p_i \rvert}{p_i}
	\end{equation}
	
	\item \textit{Heteroscedasticity-adjust Mean Square Error} (HMSE)
	\begin{equation}
	HMSE=\frac{1}{N} \sum_{i=1}^{N}[\frac{p_i}{\hat{p}_i}- 1]^2
	\end{equation}
\end{itemize}
The first four measurements is scaled by predicted volatility, which are suitable to compare the prediction of same stock during equal period. The later two can be used to compare every method as they represent percentage errors.\\


In addition to knowing the amount of changes measurement, the direction of prices is also very important. Here, corrected direction change (CDC)\cite{naeini2010stock} are used
\begin{equation}
CDC = \frac{\text{Number of Corrected Forecast Trend}}{N}
\end{equation}
A completely random prediction should have $ 50\% $ CDC, so for a reliable forecast, this value should be greater than $ 50\% $\\


This is no measurement that are perfect enough to compare all learning algorithms, all criteria would be considered as a whole. Besides, as there are some randomness in the training method, all the result would be tested 3 times and get the average.